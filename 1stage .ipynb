{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from generators import PatchGenerator, PatchSequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import multi_gpu_model\n",
    "from networks_binary import (create_initial_model,\n",
    "                      create_second_model,\n",
    "                      create_squeezenet3d_model,\n",
    "                      create_squeezenet3d_model2\n",
    "                     )\n",
    "from skimage.transform import rotate\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import keras.backend as K\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "import sys, os, warnings\n",
    "\n",
    "DATADIR = '/projects/0/ismi2018/FINALPROJECTS/BREAST_3D_ULTRASOUND/shareWithStudents'\n",
    "\n",
    "NETWORKS = {\n",
    "    'initial': create_initial_model,\n",
    "    'second': create_second_model,\n",
    "    'squeezenet3d': create_squeezenet3d_model,\n",
    "    'squeezenet3d2': create_squeezenet3d_model2\n",
    "}\n",
    "\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    'adam': Adam,\n",
    "    'rmsprop': RMSprop,\n",
    "}\n",
    "\n",
    "\n",
    "class MultiGPUCheckpoint(Callback):\n",
    "\n",
    "    def __init__(self, filename, verbose=0):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.verbose = verbose\n",
    "        self.val_accs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not self.val_accs:\n",
    "            self.model.layers[-2].save(self.filename)\n",
    "        elif logs['val_acc'] > max(self.val_accs):\n",
    "            if self.verbose > 0:\n",
    "                print('Saving to {}'.format(self.filename))\n",
    "            self.model.layers[-2].save(self.filename)\n",
    "        self.val_accs.append(logs['val_acc'])\n",
    "\n",
    "\n",
    "class Accuracies(Callback):\n",
    "\n",
    "    def __init__(self, valid_seq):\n",
    "        super().__init__()\n",
    "        self.valid_seq = valid_seq\n",
    "        self.label_accuracies = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict_generator(self.valid_seq,\n",
    "                                              workers=4,\n",
    "                                              use_multiprocessing=True)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        y_true = self.valid_seq.get_all_labels()\n",
    "        y_true[y_true == 2] = 0\n",
    "        y_true[y_true == 20] = 1\n",
    "        y_true[y_true == 21] = 2\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        ps = cm.diagonal() / cm.sum(axis=1)\n",
    "        self.label_accuracies.append(ps)\n",
    "\n",
    "\n",
    "def create_model(network, optimizer, drop_rate, multi_gpu):\n",
    "    orig_model = NETWORKS[network](drop_rate=drop_rate)\n",
    "    if multi_gpu:\n",
    "        parallel_model = multi_gpu_model(orig_model)\n",
    "        parallel_model.compile(optimizer=optimizer, loss=LOSS_FUNCTION,\n",
    "                               metrics=['accuracy'])\n",
    "    else:\n",
    "        orig_model.compile(optimizer=optimizer, loss=LOSS_FUNCTION,\n",
    "                           metrics=['accuracy'])\n",
    "        parallel_model = None\n",
    "    return orig_model, parallel_model\n",
    "\n",
    "\n",
    "def create_optimizer(name, lr, decay):\n",
    "    return OPTIMIZERS[name](lr=lr, decay=decay)\n",
    "\n",
    "\n",
    "def make_augmentation_func(aug, aug_hflip, aug_vflip, aug_rotate,\n",
    "                           aug_brightness):\n",
    "    if not aug:\n",
    "        return None\n",
    "\n",
    "    def augf(img):\n",
    "        if np.random.random() > aug and aug_hflip:\n",
    "            img = np.fliplr(img)\n",
    "        if np.random.random() > aug and aug_vflip:\n",
    "            img = np.flipud(img)\n",
    "        if np.random.random() > aug and aug_rotate:\n",
    "            tmp = np.squeeze(img)\n",
    "            angle = np.random.uniform(0, aug_rotate)\n",
    "            tmp = rotate(tmp, angle)\n",
    "            img = np.expand_dims(tmp, -1)\n",
    "        if np.random.random() > aug and aug_brightness:\n",
    "            up_delta = 1. - img.max()\n",
    "            down_delta = img.min()\n",
    "            delta = min(up_delta, down_delta)\n",
    "            img = img + np.random.uniform(-delta, delta)\n",
    "        return img[15:55, 15:55, ...]\n",
    "\n",
    "    return augf\n",
    "\n",
    "\n",
    "def make_generators(csv, train_patients, validation_patients, batch_size,\n",
    "                    augf):\n",
    "    train_csv = csv.loc[csv['patientID'].isin(train_patients), :]\n",
    "    valid_csv = csv.loc[csv['patientID'].isin(validation_patients), :]\n",
    "\n",
    "    train_gen = PatchGenerator(\n",
    "        input_dir=DATADIR,\n",
    "        dataframe=train_csv,\n",
    "        batch_size=batch_size,\n",
    "        augmentation_fn=augf\n",
    "    )\n",
    "\n",
    "    valid_seq = PatchSequence(\n",
    "        input_dir=DATADIR,\n",
    "        dataframe=valid_csv,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_gen, valid_seq\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    csv = pd.read_csv(os.path.join(DATADIR, 'trainingSet.csv'), dtype=str)\n",
    "\n",
    "    # Create patient K-folder\n",
    "    unique_patients = csv.patientID.unique()\n",
    "    kf = KFold(5, shuffle=True, random_state=42)\n",
    "    folds = kf.split(unique_patients)\n",
    "\n",
    "    # Make augmentation function\n",
    "    augf = make_augmentation_func(.5,\n",
    "                                  True,\n",
    "                                  True,\n",
    "                                  90,\n",
    "                                  True)\n",
    "\n",
    "    if False:\n",
    "        cw = compute_class_weight('balanced',\n",
    "                                  np.unique(csv['histology'].values),\n",
    "                                  csv['histology'].values)\n",
    "        cwdict = dict(enumerate(cw))\n",
    "    else:\n",
    "        cwdict = None\n",
    "\n",
    "    accuracies = []\n",
    "    for i, (train_idxs, val_idxs) in enumerate(folds, start=1):\n",
    "        K.clear_session()\n",
    "        print('Fold {}'.format(i))\n",
    "\n",
    "        train_patients = unique_patients[train_idxs]\n",
    "        val_patients = unique_patients[val_idxs]\n",
    "\n",
    "        train_gen, valid_seq = make_generators(csv,\n",
    "                                               train_patients,\n",
    "                                               val_patients,\n",
    "                                               30,\n",
    "                                               augf)\n",
    "        optimizer = create_optimizer('adam', 1e-4, 1e-6)\n",
    "        orig_net, parallel_net = create_model('squeezenet3d2', optimizer,\n",
    "                                              0.5,\n",
    "                                              True)\n",
    "\n",
    "        save_filename = '{}_fold_{}.h5'.format('1st-step', i)\n",
    "        if True:\n",
    "            cp = MultiGPUCheckpoint(save_filename, verbose=1)\n",
    "        else:\n",
    "            cp = ModelCheckpoint(save_filename, save_best_only=True, verbose=1,\n",
    "                                 monitor='val_acc')\n",
    "        ps = Accuracies(valid_seq)\n",
    "\n",
    "        train_model = parallel_net or orig_net\n",
    "        results = train_model.fit_generator(train_gen,\n",
    "                                            steps_per_epoch=len(train_gen),\n",
    "                                            validation_data=valid_seq,\n",
    "                                            epochs=50,\n",
    "                                            use_multiprocessing=True,\n",
    "                                            workers=4,\n",
    "                                            class_weight=cwdict,\n",
    "                                            callbacks=[cp, ps],\n",
    "                                            verbose=1)\n",
    "\n",
    "        h = results.history\n",
    "        plt.figure()\n",
    "        plt.plot(h['loss'])\n",
    "        plt.plot(h['acc'])\n",
    "        plt.plot(h['val_loss'])\n",
    "        plt.plot(h['val_acc'])\n",
    "        plt.legend(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "        plt.savefig('{}.traininglog.png'.format(save_filename))\n",
    "\n",
    "        y_true = valid_seq.get_all_labels()\n",
    "        y_true[y_true == 2] = 0\n",
    "        y_true[y_true == 20] = 1\n",
    "        y_true[y_true == 21] = 2\n",
    "        best_net = load_model(save_filename)\n",
    "        y_pred = best_net.predict_generator(valid_seq,\n",
    "                                            workers=4,\n",
    "                                            use_multiprocessing=True)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        plt.figure()\n",
    "        sns.heatmap(cm, annot=True)\n",
    "        plt.savefig('{}.confusionmatrix.png'.format(save_filename))\n",
    "\n",
    "        precs = np.array(ps.label_accuracies)\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(precs.shape[1]):\n",
    "            plt.plot(precs[:, i])\n",
    "        plt.legend(['0', '1', '2'])\n",
    "        plt.savefig('{}.accuracies.png'.format(save_filename))\n",
    "\n",
    "        accuracies.append(max(h['val_acc']))\n",
    "\n",
    "    with open('{}_score.txt'.format(args.filename), 'w') as f:\n",
    "        print('Mean accuracy: {:.4f}'.format(np.mean(accuracies)), file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "PatchGenerator detected: 239 patch samples.\n",
      "PatchSequence detected: 59 patch samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0027/.local/lib/python3.5/site-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_1 to have shape (2,) but got array with shape (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-9a9e4bd547ec>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                                             verbose=1)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1878\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_1 to have shape (2,) but got array with shape (3,)"
     ]
    }
   ],
   "source": [
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
