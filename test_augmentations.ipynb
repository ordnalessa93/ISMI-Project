{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "from generators import PatchGenerator, PatchSequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import multi_gpu_model\n",
    "from networks import (create_initial_model,\n",
    "                      create_second_model,\n",
    "                      create_squeezenet3d_model,\n",
    "                     )\n",
    "from skimage.transform import rotate\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import keras.backend as K\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_augmentation_func(aug            = 0.5, \n",
    "                           aug_hflip      = True, \n",
    "                           aug_vflip      = True, \n",
    "                           aug_rotate     = True, \n",
    "                           aug_brightness = True, \n",
    "                           aug_contrast   = True, \n",
    "                           aug_zoom       = True, \n",
    "                           aug_shift      = True,\n",
    "                           aug_padding    = True\n",
    "                           ):\n",
    "    if not aug:\n",
    "        return None\n",
    "    def augf(img):\n",
    "        zoomed = False\n",
    "        if np.random.random() > aug and aug_hflip:\n",
    "            img = np.flip(img, axis = 1)\n",
    "        if np.random.random() > aug and aug_vflip:\n",
    "            img = np.flip(img, axis = 0)\n",
    "        if np.random.random() > aug and aug_rotate:\n",
    "            tmp = np.squeeze(img)\n",
    "            angle = np.random.uniform(0, aug_rotate)\n",
    "            tmp = rotate(tmp, angle)\n",
    "            img = np.expand_dims(tmp, -1)\n",
    "        if np.random.random() > aug and aug_brightness:\n",
    "            up_delta = 1. - img.max()\n",
    "            down_delta = img.min()\n",
    "            delta = min(up_delta, down_delta)\n",
    "            img = img + np.random.uniform(-delta, delta)    \n",
    "        if np.random.random() > aug and aug_contrast:\n",
    "            tmp = np.reshape(img, img.shape[0], img.shape[1]*img.shape[2])\n",
    "            tmp = equalize_adapthist(tmp, kernel_size=(5,5,5,1))\n",
    "            img = np.reshape(tmp, 70, 70, 40, 1)\n",
    "\n",
    "        if np.random.random() > aug and aug_zoom:\n",
    "            img = zoom(img, (*np.random.choice([0.9, 1.1], 2), 1,1))\n",
    "            zoomed = True\n",
    "\n",
    "        crop_from = [15, 15]\n",
    "        if np.random.random() > aug and aug_shift:\n",
    "            if zoomed:\n",
    "                crop_from = np.random.randint(0, 23, 2)\n",
    "            else:\n",
    "                crop_from = np.random.randint(0, 30, 2)\n",
    "\n",
    "        # Padding is only over cropped image\n",
    "        img = img[crop_from[0]:crop_from[0] + 40, crop_from[1]:crop_from[1] + 40, :]\n",
    "        \n",
    "        if np.random.random() > aug and aug_padding:\n",
    "            try:\n",
    "                direction = np.random.randint(0,2)\n",
    "                start = np.random.choice([0,20])\n",
    "                size = np.random.randint(0,20)\n",
    "                if direction == 0:\n",
    "                    img[start:size+start, :, :] = 0\n",
    "                elif direction == 1:\n",
    "                    img[:, start:size+start, :] = 0\n",
    "                else:\n",
    "                    img[:,:, start:size+start] = 0\n",
    "            except Exception:\n",
    "                print(\"padding augmentation failed\")\n",
    "            \n",
    "        return img\n",
    "\n",
    "    return augf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single stage training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORKS = {\n",
    "    'initial': create_initial_model,\n",
    "    'second': create_second_model,\n",
    "    'squeezenet3d': create_squeezenet3d_model,\n",
    "}\n",
    "\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    'adam': Adam,\n",
    "    'rmsprop': RMSprop,\n",
    "}\n",
    "\n",
    "DATADIR = '/projects/0/ismi2018/FINALPROJECTS/BREAST_3D_ULTRASOUND/shareWithStudents'\n",
    "\n",
    "class MultiGPUCheckpoint(Callback):\n",
    "\n",
    "    def __init__(self, filename, verbose=0):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.verbose = verbose\n",
    "        self.val_accs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not self.val_accs:\n",
    "            self.model.layers[-2].save(self.filename)\n",
    "        elif logs['val_acc'] > max(self.val_accs):\n",
    "            if self.verbose > 0:\n",
    "                print('Saving to {}'.format(self.filename))\n",
    "            self.model.layers[-2].save(self.filename)\n",
    "        self.val_accs.append(logs['val_acc'])\n",
    "\n",
    "\n",
    "class Accuracies(Callback):\n",
    "\n",
    "    def __init__(self, valid_seq):\n",
    "        super().__init__()\n",
    "        self.valid_seq = valid_seq\n",
    "        self.label_accuracies = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict_generator(self.valid_seq,\n",
    "                                              workers=4,\n",
    "                                              use_multiprocessing=True)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        y_true = self.valid_seq.get_all_labels()\n",
    "        y_true[y_true == 2] = 0\n",
    "        y_true[y_true == 20] = 1\n",
    "        y_true[y_true == 21] = 2\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        ps = cm.diagonal() / cm.sum(axis=1)\n",
    "        self.label_accuracies.append(ps)\n",
    "\n",
    "\n",
    "def create_model(network, optimizer, drop_rate, multi_gpu):\n",
    "    orig_model = NETWORKS[network](drop_rate=drop_rate)\n",
    "    if multi_gpu:\n",
    "        parallel_model = multi_gpu_model(orig_model)\n",
    "        parallel_model.compile(optimizer=optimizer, loss=LOSS_FUNCTION,\n",
    "                               metrics=['accuracy'])\n",
    "    else:\n",
    "        orig_model.compile(optimizer=optimizer, loss=LOSS_FUNCTION,\n",
    "                           metrics=['accuracy'])\n",
    "        parallel_model = None\n",
    "    return orig_model, parallel_model\n",
    "\n",
    "\n",
    "def create_optimizer(name, lr, decay):\n",
    "    return OPTIMIZERS[name](lr=lr, decay=decay)\n",
    "\n",
    "\n",
    "def make_generators(csv, train_patients, validation_patients, batch_size,\n",
    "                    augf):\n",
    "    train_csv = csv.loc[csv['patientID'].isin(train_patients), :]\n",
    "    valid_csv = csv.loc[csv['patientID'].isin(validation_patients), :]\n",
    "\n",
    "    train_gen = PatchGenerator(\n",
    "        input_dir=DATADIR,\n",
    "        dataframe=train_csv,\n",
    "        batch_size=batch_size,\n",
    "        augmentation_fn=augf\n",
    "    )\n",
    "\n",
    "    valid_seq = PatchSequence(\n",
    "        input_dir=DATADIR,\n",
    "        dataframe=valid_csv,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_gen, valid_seq\n",
    "\n",
    "\n",
    "def train_model(augmentations, filename):\n",
    "    csv = pd.read_csv(os.path.join(DATADIR, 'trainingSet.csv'), dtype=str)\n",
    "\n",
    "\n",
    "    # Create patient K-folder\n",
    "    unique_patients = csv.patientID.unique()\n",
    "    kf = KFold(5, shuffle=True, random_state=42)\n",
    "    folds = kf.split(unique_patients)\n",
    "\n",
    "    # Make augmentation function\n",
    "    augf = make_augmentation_func(*augmentations)\n",
    "\n",
    "    class_weight = False\n",
    "    if class_weight:\n",
    "        cw = compute_class_weight('balanced',\n",
    "                                  np.unique(csv['histology'].values),\n",
    "                                  csv['histology'].values)\n",
    "        cwdict = dict(enumerate(cw))\n",
    "    else:\n",
    "        cwdict = None\n",
    "\n",
    "    accuracies = []\n",
    "    for i, (train_idxs, val_idxs) in enumerate(folds, start=1):\n",
    "        K.clear_session()\n",
    "        print('Fold {}'.format(i))\n",
    "\n",
    "        train_patients = unique_patients[train_idxs]\n",
    "        print(train_patients)\n",
    "        val_patients = unique_patients[val_idxs]\n",
    "\n",
    "        train_gen, valid_seq = make_generators(csv,\n",
    "                                               train_patients,\n",
    "                                               val_patients,\n",
    "                                               6,\n",
    "                                               augf)\n",
    "        print(train_gen)\n",
    "        optimizer = create_optimizer('adam', 1e-5, 1e-6)\n",
    "                \n",
    "        orig_net, parallel_net = create_model('squeezenet3d', optimizer,\n",
    "                                              0.5,\n",
    "                                              True)\n",
    "\n",
    "        save_filename = '{}_fold_{}.h5'.format(filename, i)\n",
    "        \n",
    "        if True:\n",
    "            cp = MultiGPUCheckpoint(save_filename, verbose=1)\n",
    "        else:\n",
    "            cp = ModelCheckpoint(save_filename, save_best_only=True, verbose=1,\n",
    "                                 monitor='val_acc')\n",
    "        ps = Accuracies(valid_seq)\n",
    "\n",
    "        train_model = parallel_net or orig_net\n",
    "        results = train_model.fit_generator(train_gen,\n",
    "                                            steps_per_epoch=len(train_gen),\n",
    "                                            validation_data=valid_seq,\n",
    "                                            epochs=50,\n",
    "                                            use_multiprocessing=True,\n",
    "                                            workers=4,\n",
    "                                            class_weight=cwdict,\n",
    "                                            callbacks=[cp, ps],\n",
    "                                            verbose=1)\n",
    "\n",
    "        h = results.history\n",
    "        plt.figure()\n",
    "        plt.plot(h['loss'])\n",
    "        plt.plot(h['acc'])\n",
    "        plt.plot(h['val_loss'])\n",
    "        plt.plot(h['val_acc'])\n",
    "        plt.legend(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "        plt.savefig('{}.traininglog.png'.format(save_filename))\n",
    "\n",
    "        y_true = valid_seq.get_all_labels()\n",
    "        y_true[y_true == 2] = 0\n",
    "        y_true[y_true == 20] = 1\n",
    "        y_true[y_true == 21] = 2\n",
    "        best_net = load_model(save_filename)\n",
    "        y_pred = best_net.predict_generator(valid_seq,\n",
    "                                            workers=4,\n",
    "                                            use_multiprocessing=True)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        plt.figure()\n",
    "        sns.heatmap(cm, annot=True)\n",
    "        plt.savefig('{}.confusionmatrix.png'.format(save_filename))\n",
    "\n",
    "        precs = np.array(ps.label_accuracies)\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(precs.shape[1]):\n",
    "            plt.plot(precs[:, i])\n",
    "        plt.legend(['0', '1', '2'])\n",
    "        plt.savefig('{}.accuracies.png'.format(save_filename))\n",
    "\n",
    "        accuracies.append(max(h['val_acc']))\n",
    "\n",
    "    with open('{}_score.txt'.format(filename), 'w') as f:\n",
    "        print('Mean accuracy: {:.4f}'.format(np.mean(accuracies)), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "['02000801' '02002243' '02002287' '02002117' '02001283' '02004004'\n",
      " '02001241' '02000699' '02001238' '02001798' '02000740' '02000757'\n",
      " '02001839' '02002118' '02004362' '02000735' '02001746' '02004189'\n",
      " '02000731' '02001951' '02002166' '02000903' '02004418' '02002527'\n",
      " '02004437' '02000751' '02002560' '02001909' '02009241' '02001305'\n",
      " '02000500' '02001293' '02001877' '02001881' '02001233' '02001813'\n",
      " '02001743' '02002076' '02004171' '02001830' '02004033' '02008917'\n",
      " '02006939' '02004427' '02001312' '02002238' '02001560' '02001261'\n",
      " '02001818' '02000714' '02000739' '02004475' '02000736' '02000729'\n",
      " '02001791' '02000678' '02009188' '02002069' '02000688' '02001386'\n",
      " '02001774' '02002545' '02000704' '02002800' '02004214' '02000727'\n",
      " '02000081' '02001833' '02001254' '02000703' '02002171' '02001944'\n",
      " '02000765' '02001285' '02001167' '02009163' '02001313' '02000289'\n",
      " '02002020' '02004242' '02000732' '02000838' '02002563' '02001276'\n",
      " '02004028' '02002540' '02000764' '02002132' '02000758' '02004258'\n",
      " '02004170' '02004192' '02001923' '02001810' '02001796' '02002158'\n",
      " '02001735' '02001787' '02000743' '02001808' '02007209' '02008881'\n",
      " '02000854' '02001289' '02004160' '02001822' '02001577' '02001821'\n",
      " '02000766' '02000700' '02002165' '02009179']\n",
      "PatchGenerator detected: 239 patch samples.\n",
      "PatchSequence detected: 59 patch samples.\n",
      "<generators.PatchGenerator object at 0x2ab62b1e4eb8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0027/.local/lib/python3.5/site-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Failed reading idx 160...\n",
      "Failed reading idx 136...\n",
      "Failed reading idx 279...\n",
      "Failed reading idx 150...\n",
      "Failed reading idx 297...\n",
      "Failed reading idx 9...\n",
      "Failed reading idx 210...\n",
      "Failed reading idx 84...\n",
      "Failed reading idx 217...\n",
      "Failed reading idx 147...\n",
      "Failed reading idx 219...\n",
      "Failed reading idx 218...\n",
      "Failed reading idx 218...\n",
      "Failed reading idx 6...\n",
      "Failed reading idx 144...\n",
      "Failed reading idx 234...\n",
      "Failed reading idx 203...\n",
      "Failed reading idx 288...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/nfs/home1/ruc0027/ISMI-Project/generators.py\", line 77, in next\n",
      "    batch_x = np.stack(images).astype(K.floatx())\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ruc0027/.local/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 677, in _data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"/nfs/home1/ruc0027/ISMI-Project/generators.py\", line 31, in __next__\n",
      "    return self.next()\n",
      "  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/numpy/core/shape_base.py\", line 343, in stack\n",
      "    raise ValueError('need at least one array to stack')\n",
      "ValueError: need at least one array to stack\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed reading idx 94...\n",
      "Failed reading idx 10...\n",
      "Failed reading idx 244...\n",
      "Failed reading idx 228...\n",
      "Failed reading idx 147...\n",
      "Failed reading idx 266...\n",
      "Failed reading idx 216...\n",
      "Failed reading idx 78...\n",
      "Failed reading idx 210...\n",
      "Failed reading idx 26...\n",
      "Failed reading idx 92...\n",
      "Failed reading idx 123...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ruc0027/.local/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 677, in _data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"/nfs/home1/ruc0027/ISMI-Project/generators.py\", line 31, in __next__\n",
      "    return self.next()\n",
      "  File \"/nfs/home1/ruc0027/ISMI-Project/generators.py\", line 77, in next\n",
      "    batch_x = np.stack(images).astype(K.floatx())\n",
      "  File \"/hpc/sw/python-3.5.2/lib/python3.5/site-packages/numpy/core/shape_base.py\", line 343, in stack\n",
      "    raise ValueError('need at least one array to stack')\n",
      "ValueError: need at least one array to stack\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed reading idx 39...\n",
      "Failed reading idx 26...\n"
     ]
    }
   ],
   "source": [
    "# aug, aug_hflip, aug_vflip, aug_rotate, aug_brightness, aug_contrast, aug_zoom, aug_shift\n",
    "\n",
    "# No augmentation\n",
    "augs = (False,)\n",
    "train_model(augs, \"no_augs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only padding\n",
    "# augs = (0.5, False, False, False, False, False, False, False, True)\n",
    "# train_model(augs, \"padding_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only flipping and rotation\n",
    "# augs = (0.5, True, True, True, False, False, False, False, True)\n",
    "# train_model(augs, \"padding_flip_rot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only brightness\n",
    "# augs = (0.5, False, False, False, True, False, False, False, True)\n",
    "# train_model(augs, \"padding_brightness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only contrast\n",
    "# augs = (0.5, False, False, False, False, True, False, False, True)\n",
    "# train_model(augs, \"padding_contrast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only zoom\n",
    "# augs = (0.5, False, False, False, False, False, True, False, True)\n",
    "# train_model(augs, \"padding_zoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All augmentations\n",
    "# augs = (0.5, True, True, True, True, True, True, False, True)\n",
    "# train_model(augs, \"padding_all_augs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only padding\n",
    "augs = (0.5, False, False, False, False, False, False, True, False)\n",
    "train_model(augs, \"shift_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only flipping and rotation\n",
    "augs = (0.5, True, True, True, False, False, False, True, False)\n",
    "train_model(augs, \"shift_flip_rot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only brightness\n",
    "augs = (0.5, False, False, False, True, False, False, True, False)\n",
    "train_model(augs, \"shift_brightness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only contrast\n",
    "augs = (0.5, False, False, False, False, True, False, True, False)\n",
    "train_model(augs, \"shift_contrast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only zoom\n",
    "augs = (0.5, False, False, False, False, False, True, True, False)\n",
    "train_model(augs, \"shift_zoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All augmentations\n",
    "augs = (0.5, True, True, True, True, True, True, True, False)\n",
    "train_model(augs, \"shift_all_augs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e2e14706e145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_device_placement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \"\"\"\n\u001b[0;32m-> 1482\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    620\u001b[0m           \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewDeprecatedSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteSessionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "from generators_binary import PatchGenerator, PatchSequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import multi_gpu_model\n",
    "from networks_binary import (create_initial_model,\n",
    "                      create_second_model,\n",
    "                      create_squeezenet3d_model,\n",
    "                      create_squeezenet3d_model2\n",
    "                     )\n",
    "from skimage.transform import rotate\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# machine learning / deep learning\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "import keras.backend as K\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys, os, warnings\n",
    "\n",
    "DATADIR = '/projects/0/ismi2018/FINALPROJECTS/BREAST_3D_ULTRASOUND/shareWithStudents'\n",
    "\n",
    "NETWORKS = {\n",
    "    'initial': create_initial_model,\n",
    "    'second': create_second_model,\n",
    "    'squeezenet3d': create_squeezenet3d_model,\n",
    "    'squeezenet3d2': create_squeezenet3d_model2\n",
    "}\n",
    "\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    'adam': Adam,\n",
    "    'rmsprop': RMSprop,\n",
    "}\n",
    "\n",
    "\n",
    "class MultiGPUCheckpoint(Callback):\n",
    "\n",
    "    def __init__(self, filename, verbose=0):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.verbose = verbose\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not self.val_loss:\n",
    "            self.model.layers[-2].save(self.filename)\n",
    "        elif logs['val_loss'] < min(self.val_loss):\n",
    "            if self.verbose > 0:\n",
    "                print('Saving to {}'.format(self.filename))\n",
    "            self.model.layers[-2].save(self.filename)\n",
    "        self.val_loss.append(logs['val_loss'])\n",
    "\n",
    "\n",
    "class Accuracies(Callback):\n",
    "\n",
    "    def __init__(self, valid_seq, step = 0):\n",
    "        super().__init__()\n",
    "        self.valid_seq = valid_seq\n",
    "        self.label_accuracies = []\n",
    "        self.step = step\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        y_pred = self.model.predict_generator(self.valid_seq,\n",
    "                                              workers=4,\n",
    "                                              use_multiprocessing=True)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = self.valid_seq.get_all_labels()\n",
    "        \n",
    "        if self.step > 0:\n",
    "            y_true = y_true[y_true != 21]\n",
    "            y_true[y_true == 2] = 1\n",
    "            y_true[y_true == 20] = 0\n",
    "        else:\n",
    "            y_true[y_true == 2] = 1\n",
    "            y_true[y_true == 20] = 1\n",
    "            y_true[y_true == 21] = 0\n",
    "        \n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        ps = cm.diagonal() / cm.sum(axis=1)\n",
    "        self.label_accuracies.append(ps)\n",
    "\n",
    "\n",
    "def create_model(network, optimizer, drop_rate, multi_gpu):\n",
    "    orig_model = NETWORKS[network](drop_rate=drop_rate)\n",
    "    if multi_gpu:\n",
    "        parallel_model = multi_gpu_model(orig_model)\n",
    "        parallel_model.compile(optimizer=optimizer, loss=LOSS_FUNCTION,\n",
    "                               metrics=['accuracy'])\n",
    "    else:\n",
    "        orig_model.compile(optimizer=optimizer, loss=LOSS_FUNCTION,\n",
    "                           metrics=['accuracy'])\n",
    "        parallel_model = None\n",
    "    return orig_model, parallel_model\n",
    "\n",
    "\n",
    "def create_optimizer(name, lr, decay):\n",
    "    return OPTIMIZERS[name](lr=lr, decay=decay)\n",
    "\n",
    "\n",
    "def make_generators(csv, train_patients, validation_patients, batch_size, step,\n",
    "                    augf):\n",
    "    train_csv = csv.loc[csv['patientID'].isin(train_patients), :]\n",
    "    valid_csv = csv.loc[csv['patientID'].isin(validation_patients), :]\n",
    "\n",
    "    train_gen = PatchGenerator(\n",
    "        input_dir=DATADIR,\n",
    "        dataframe=train_csv,\n",
    "        batch_size=batch_size,\n",
    "        step=step,\n",
    "        augmentation_fn=augf\n",
    "    )\n",
    "\n",
    "    valid_seq = PatchSequence(\n",
    "        input_dir=DATADIR,\n",
    "        dataframe=valid_csv,\n",
    "        batch_size=batch_size,\n",
    "        step=step\n",
    "    )\n",
    "\n",
    "    return train_gen, valid_seq\n",
    "\n",
    "\n",
    "def train_model_1st(augmentations, filename):\n",
    "    csv = pd.read_csv(os.path.join(DATADIR, 'trainingSet.csv'), dtype=str)\n",
    "\n",
    "    # Create patient K-folder\n",
    "    unique_patients = csv.patientID.unique()\n",
    "    kf = KFold(5, shuffle=True, random_state=42)\n",
    "    folds = kf.split(unique_patients)\n",
    "\n",
    "    # Make augmentation function\n",
    "    augf = make_augmentation_func(*augmentations)\n",
    "    \n",
    "    # -----------------------------------------------------------------------\n",
    "        \n",
    "    accuracies_first = []\n",
    "    # first step training\n",
    "    for i, (train_idxs, val_idxs) in enumerate(folds, start=1):\n",
    "        K.clear_session()\n",
    "        print('Fold {}'.format(i))\n",
    "\n",
    "        train_patients = unique_patients[train_idxs]\n",
    "        val_patients = unique_patients[val_idxs]\n",
    "\n",
    "        train_gen, valid_seq = make_generators(csv,\n",
    "                                               train_patients,\n",
    "                                               val_patients,\n",
    "                                               6,\n",
    "                                               0,\n",
    "                                               augf)\n",
    "        \n",
    "        optimizer = create_optimizer('adam', 1e-5, 1e-6)\n",
    "        orig_net, parallel_net = create_model('squeezenet3d', optimizer,\n",
    "                                              0.5,\n",
    "                                              True)\n",
    "\n",
    "        save_filename = './results_1st/{}_fold_{}.h5'.format(filename, i)\n",
    "        \n",
    "        if True:\n",
    "            cp = MultiGPUCheckpoint(save_filename, verbose=1)\n",
    "        else:\n",
    "            cp = ModelCheckpoint(save_filename, save_best_only=True, verbose=1,\n",
    "                                 monitor='val_acc')\n",
    "        ps = Accuracies(valid_seq, 0)\n",
    "\n",
    "        train_model = parallel_net or orig_net\n",
    "                \n",
    "        results = train_model.fit_generator(train_gen,\n",
    "                                            steps_per_epoch=len(train_gen),\n",
    "                                            validation_data=valid_seq,\n",
    "                                            epochs=100,\n",
    "                                            workers=4,\n",
    "                                            use_multiprocessing=True,\n",
    "                                            callbacks=[cp, ps],\n",
    "                                            verbose=1)\n",
    "\n",
    "        h = results.history\n",
    "        plt.figure()\n",
    "        plt.plot(h['loss'])\n",
    "        plt.plot(h['acc'])\n",
    "        plt.plot(h['val_loss'])\n",
    "        plt.plot(h['val_acc'])\n",
    "        plt.legend(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "        plt.savefig('{}.traininglog.png'.format(save_filename))\n",
    "\n",
    "        y_true = valid_seq.get_all_labels()\n",
    "        y_true[y_true == 2] = 1\n",
    "        y_true[y_true == 20] = 1\n",
    "        y_true[y_true == 21] = 0\n",
    "        best_net = load_model(save_filename)\n",
    "        y_pred = best_net.predict_generator(valid_seq,\n",
    "                                            workers=4,\n",
    "                                            use_multiprocessing=True)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        plt.figure()\n",
    "        sns.heatmap(cm, annot=True)\n",
    "        plt.savefig('{}.confusionmatrix.png'.format(save_filename))\n",
    "\n",
    "        precs = np.array(ps.label_accuracies)\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(precs.shape[1]):\n",
    "            plt.plot(precs[:, i])\n",
    "        plt.legend(['0', '1'])\n",
    "        plt.savefig('{}.accuracies.png'.format(save_filename))\n",
    "\n",
    "        accuracies_first.append(max(h['val_acc']))\n",
    "\n",
    "    with open('{}_score.txt'.format(filename), 'w') as f:\n",
    "        print('Mean accuracy 1st step: {:.4f}\\n'.format(np.mean(accuracies_first)), file=f)\n",
    "        \n",
    "\n",
    "def train_model_2nd(augmentations, filename):\n",
    "    csv = pd.read_csv(os.path.join(DATADIR, 'trainingSet.csv'), dtype=str)\n",
    "\n",
    "    # Create patient K-folder\n",
    "    unique_patients = csv.patientID.unique()\n",
    "    kf = KFold(5, shuffle=True, random_state=42)\n",
    "    folds = kf.split(unique_patients)\n",
    "\n",
    "    # Make augmentation function\n",
    "    augf = make_augmentation_func(*augmentations)\n",
    "                \n",
    "    # -------------------------------------------------------------------------\n",
    "    accuracies_second = []\n",
    "    # second step training\n",
    "    for i, (train_idxs, val_idxs) in enumerate(folds, start=1):\n",
    "        K.clear_session()\n",
    "        print('Fold {}'.format(i))\n",
    "\n",
    "        train_patients = unique_patients[train_idxs]\n",
    "        val_patients = unique_patients[val_idxs]\n",
    "\n",
    "        train_gen, valid_seq = make_generators(csv,\n",
    "                                               train_patients,\n",
    "                                               val_patients,\n",
    "                                               6,\n",
    "                                               1,\n",
    "                                               augf)\n",
    "        \n",
    "        optimizer = create_optimizer('adam', 1e-5, 1e-6)\n",
    "        orig_net, parallel_net = create_model('squeezenet3d', optimizer,\n",
    "                                              0.5,\n",
    "                                              True)\n",
    "\n",
    "        save_filename = './results_2nd/{}_fold_{}.h5'.format(filename, i)\n",
    "        if True:\n",
    "            cp = MultiGPUCheckpoint(save_filename, verbose=1)\n",
    "        else:\n",
    "            cp = ModelCheckpoint(save_filename, save_best_only=True, verbose=1,\n",
    "                                 monitor='val_acc')\n",
    "        ps = Accuracies(valid_seq, 1)\n",
    "\n",
    "        train_model = parallel_net or orig_net\n",
    "                \n",
    "        results = train_model.fit_generator(train_gen,\n",
    "                                            steps_per_epoch=len(train_gen),\n",
    "                                            validation_data=valid_seq,\n",
    "                                            epochs=100,\n",
    "                                            workers=4,\n",
    "                                            use_multiprocessing=True,\n",
    "                                            callbacks=[cp, ps],\n",
    "                                            verbose=1)\n",
    "\n",
    "        h = results.history\n",
    "        plt.figure()\n",
    "        plt.plot(h['loss'])\n",
    "        plt.plot(h['acc'])\n",
    "        plt.plot(h['val_loss'])\n",
    "        plt.plot(h['val_acc'])\n",
    "        plt.legend(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "        plt.savefig('{}.traininglog.png'.format(save_filename))\n",
    "\n",
    "        y_true = valid_seq.get_all_labels()\n",
    "        y_true = y_true[y_true != 21]\n",
    "        y_true[y_true == 2] = 1\n",
    "        y_true[y_true == 20] = 0\n",
    "        best_net = load_model(save_filename)\n",
    "        y_pred = best_net.predict_generator(valid_seq,\n",
    "                                            workers=4,\n",
    "                                            use_multiprocessing=True)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        plt.figure()\n",
    "        sns.heatmap(cm, annot=True)\n",
    "        plt.savefig('{}.confusionmatrix.png'.format(save_filename))\n",
    "\n",
    "        precs = np.array(ps.label_accuracies)\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(precs.shape[1]):\n",
    "            plt.plot(precs[:, i])\n",
    "        plt.legend(['0', '1'])\n",
    "        plt.savefig('{}.accuracies.png'.format(save_filename))\n",
    "\n",
    "        accuracies_second.append(max(h['val_acc']))\n",
    "    \n",
    "    with open('{}_score.txt'.format(filename), 'w') as f:\n",
    "        print('Mean accuracy 1st step: {:.4f}\\n'.format(np.mean(accuracies_second)), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug, aug_hflip, aug_vflip, aug_rotate, aug_brightness, aug_contrast, aug_zoom, aug_shift\n",
    "\n",
    "# No augmentation\n",
    "augs = (False,)\n",
    "train_model_1st(augs, \"no_augs_st1\")\n",
    "train_model_2nd(augs, \"no_augs_st2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only padding\n",
    "# augs = (0.5, False, False, False, False, False, False, False, True)\n",
    "# train_model_1st(augs, \"padding_only_st1\")\n",
    "# train_model_2nd(augs, \"padding_only_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only flipping and rotation\n",
    "# augs = (0.5, True, True, True, False, False, False, False, True)\n",
    "# train_model_1st(augs, \"padding_flip_rot_st1\")\n",
    "# train_model_2nd(augs, \"padding_flip_rot_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only brightness\n",
    "# augs = (0.5, False, False, False, True, False, False, False, True)\n",
    "# train_model_1st(augs, \"padding_brightness_st1\")\n",
    "# train_model_2nd(augs, \"padding_brightness_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only contrast\n",
    "# augs = (0.5, False, False, False, False, True, False, False, True)\n",
    "# train_model_1st(augs, \"padding_contrast_st1\")\n",
    "# train_model_2nd(augs, \"padding_contrast_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only zoom\n",
    "# augs = (0.5, False, False, False, False, False, True, False, True)\n",
    "# train_model_1st(augs, \"padding_zoom_st1\")\n",
    "# train_model_2nd(augs, \"padding_zoom_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All augmentations\n",
    "# augs = (0.5, True, True, True, True, True, True, False, True)\n",
    "# train_model_1st(augs, \"padding_all_augs_st1\")\n",
    "# train_model_2nd(augs, \"padding_all_augs_st2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only padding\n",
    "augs = (0.5, False, False, False, False, False, False, True, False)\n",
    "train_model_1st(augs, \"shift_only_st1\")\n",
    "train_model_2nd(augs, \"shift_only_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only flipping and rotation\n",
    "augs = (0.5, True, True, True, False, False, False, True, False)\n",
    "train_model_1st(augs, \"shift_flip_rot_st1\")\n",
    "train_model_2nd(augs, \"shift_flip_rot_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only brightness\n",
    "augs = (0.5, False, False, False, True, False, False, True, False)\n",
    "train_model_1st(augs, \"shift_brightness_st1\")\n",
    "train_model_2nd(augs, \"shift_brightness_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only contrast\n",
    "augs = (0.5, False, False, False, False, True, False, True, False)\n",
    "train_model_1st(augs, \"shift_contrast_st1\")\n",
    "train_model_2nd(augs, \"shift_contrast_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only zoom\n",
    "augs = (0.5, False, False, False, False, False, True, True, False)\n",
    "train_model_1st(augs, \"shift_zoom_st1\")\n",
    "train_model_2nd(augs, \"shift_zoom_st2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All augmentations\n",
    "augs = (0.5, True, True, True, True, True, True, True, False)\n",
    "train_model_1st(augs, \"shift_all_augs_st1\")\n",
    "train_model_2nd(augs, \"shift_all_augs_st2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
